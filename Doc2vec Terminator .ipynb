{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the import and libraries used\n",
    "\n",
    "from newspaper import Article\n",
    "import random\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import nltk\n",
    "import numpy as np\n",
    "import warnings\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "import multiprocessing\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ayaco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ayaco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = []\n",
    "with open(\"train_docs.txt\", errors='ignore') as f:\n",
    "    corpus_train = f.readlines()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "{33: None, 34: None, 35: None, 36: None, 37: None, 38: None, 39: None, 40: None, 41: None, 42: None, 43: None, 44: None, 45: None, 46: None, 47: None, 58: None, 59: None, 60: None, 61: None, 62: None, 63: None, 64: None, 91: None, 92: None, 93: None, 94: None, 95: None, 96: None, 123: None, 124: None, 125: None, 126: None}\n"
     ]
    }
   ],
   "source": [
    "remove_punc = dict( (ord(punct),None) for punct in string.punctuation)\n",
    "print(string.punctuation)\n",
    "print(remove_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None \n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def tsne_plot(model):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-06 13:22:16,696 : INFO : collecting all words and their counts\n",
      "2019-12-06 13:22:16,697 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2019-12-06 13:22:16,751 : INFO : collected 28613 word types and 5000 unique tags from a corpus of 5000 examples and 159102 words\n",
      "2019-12-06 13:22:16,751 : INFO : Loading a fresh vocabulary\n",
      "2019-12-06 13:22:16,789 : INFO : effective_min_count=1 retains 28613 unique words (100% of original 28613, drops 0)\n",
      "2019-12-06 13:22:16,789 : INFO : effective_min_count=1 leaves 159102 word corpus (100% of original 159102, drops 0)\n",
      "2019-12-06 13:22:16,879 : INFO : deleting the raw counts dictionary of 28613 items\n",
      "2019-12-06 13:22:16,880 : INFO : sample=1e-05 downsamples 3120 most-common words\n",
      "2019-12-06 13:22:16,881 : INFO : downsampling leaves estimated 56757 word corpus (35.7% of prior 159102)\n",
      "2019-12-06 13:22:16,932 : INFO : estimated required memory for 28613 words and 300 dimensions: 88977700 bytes\n",
      "2019-12-06 13:22:16,933 : INFO : resetting layer weights\n",
      "2019-12-06 13:22:23,479 : INFO : training model with 1 workers on 28614 vocabulary and 300 features, using sg=1 hs=0 sample=1e-05 negative=5 window=15\n",
      "2019-12-06 13:22:24,548 : INFO : EPOCH 1 - PROGRESS: at 73.14% examples, 43340 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:22:24,900 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:22:24,901 : INFO : EPOCH - 1 : training on 159102 raw words (61814 effective words) took 1.4s, 43510 effective words/s\n",
      "2019-12-06 13:22:25,964 : INFO : EPOCH 2 - PROGRESS: at 73.14% examples, 43581 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:22:26,315 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:22:26,315 : INFO : EPOCH - 2 : training on 159102 raw words (61810 effective words) took 1.4s, 43722 effective words/s\n",
      "2019-12-06 13:22:27,373 : INFO : EPOCH 3 - PROGRESS: at 73.14% examples, 43766 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:22:27,721 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:22:27,721 : INFO : EPOCH - 3 : training on 159102 raw words (61774 effective words) took 1.4s, 43986 effective words/s\n",
      "2019-12-06 13:22:28,787 : INFO : EPOCH 4 - PROGRESS: at 73.14% examples, 43473 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:22:29,143 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:22:29,144 : INFO : EPOCH - 4 : training on 159102 raw words (61934 effective words) took 1.4s, 43561 effective words/s\n",
      "2019-12-06 13:22:30,214 : INFO : EPOCH 5 - PROGRESS: at 73.14% examples, 43125 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:22:30,567 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:22:30,568 : INFO : EPOCH - 5 : training on 159102 raw words (61737 effective words) took 1.4s, 43366 effective words/s\n",
      "2019-12-06 13:22:31,626 : INFO : EPOCH 6 - PROGRESS: at 73.14% examples, 43751 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:22:31,976 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:22:31,976 : INFO : EPOCH - 6 : training on 159102 raw words (61740 effective words) took 1.4s, 43903 effective words/s\n",
      "2019-12-06 13:22:33,033 : INFO : EPOCH 7 - PROGRESS: at 73.14% examples, 43589 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:22:33,385 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:22:33,386 : INFO : EPOCH - 7 : training on 159102 raw words (61665 effective words) took 1.4s, 43759 effective words/s\n",
      "2019-12-06 13:22:34,455 : INFO : EPOCH 8 - PROGRESS: at 73.14% examples, 43271 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:22:34,807 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:22:34,808 : INFO : EPOCH - 8 : training on 159102 raw words (61785 effective words) took 1.4s, 43469 effective words/s\n",
      "2019-12-06 13:22:35,866 : INFO : EPOCH 9 - PROGRESS: at 73.14% examples, 43644 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:22:36,218 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:22:36,218 : INFO : EPOCH - 9 : training on 159102 raw words (61724 effective words) took 1.4s, 43819 effective words/s\n",
      "2019-12-06 13:22:37,283 : INFO : EPOCH 10 - PROGRESS: at 73.14% examples, 43639 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:22:37,631 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:22:37,632 : INFO : EPOCH - 10 : training on 159102 raw words (62050 effective words) took 1.4s, 43892 effective words/s\n",
      "2019-12-06 13:22:38,685 : INFO : EPOCH 11 - PROGRESS: at 73.14% examples, 44009 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-06 13:22:39,043 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:22:39,044 : INFO : EPOCH - 11 : training on 159102 raw words (61775 effective words) took 1.4s, 43816 effective words/s\n",
      "2019-12-06 13:22:40,105 : INFO : EPOCH 12 - PROGRESS: at 66.38% examples, 39840 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:22:40,547 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:22:40,548 : INFO : EPOCH - 12 : training on 159102 raw words (61852 effective words) took 1.5s, 41134 effective words/s\n",
      "2019-12-06 13:22:41,607 : INFO : EPOCH 13 - PROGRESS: at 73.14% examples, 43535 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:22:41,963 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:22:41,963 : INFO : EPOCH - 13 : training on 159102 raw words (61797 effective words) took 1.4s, 43692 effective words/s\n",
      "2019-12-06 13:22:43,024 : INFO : EPOCH 14 - PROGRESS: at 73.14% examples, 43505 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:22:43,375 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:22:43,376 : INFO : EPOCH - 14 : training on 159102 raw words (61687 effective words) took 1.4s, 43718 effective words/s\n",
      "2019-12-06 13:22:44,427 : INFO : EPOCH 15 - PROGRESS: at 73.14% examples, 43906 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:22:44,777 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:22:44,777 : INFO : EPOCH - 15 : training on 159102 raw words (61711 effective words) took 1.4s, 44058 effective words/s\n",
      "2019-12-06 13:22:45,842 : INFO : EPOCH 16 - PROGRESS: at 73.14% examples, 43369 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:22:46,193 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:22:46,194 : INFO : EPOCH - 16 : training on 159102 raw words (61666 effective words) took 1.4s, 43562 effective words/s\n",
      "2019-12-06 13:22:47,256 : INFO : EPOCH 17 - PROGRESS: at 73.14% examples, 43586 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:22:47,608 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:22:47,609 : INFO : EPOCH - 17 : training on 159102 raw words (61837 effective words) took 1.4s, 43741 effective words/s\n",
      "2019-12-06 13:22:48,669 : INFO : EPOCH 18 - PROGRESS: at 73.14% examples, 43591 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:22:49,020 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:22:49,021 : INFO : EPOCH - 18 : training on 159102 raw words (61730 effective words) took 1.4s, 43744 effective words/s\n",
      "2019-12-06 13:22:50,082 : INFO : EPOCH 19 - PROGRESS: at 73.14% examples, 43590 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:22:50,432 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:22:50,433 : INFO : EPOCH - 19 : training on 159102 raw words (61789 effective words) took 1.4s, 43789 effective words/s\n",
      "2019-12-06 13:22:51,499 : INFO : EPOCH 20 - PROGRESS: at 73.14% examples, 43490 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:22:51,850 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:22:51,851 : INFO : EPOCH - 20 : training on 159102 raw words (61909 effective words) took 1.4s, 43693 effective words/s\n",
      "2019-12-06 13:22:52,908 : INFO : EPOCH 21 - PROGRESS: at 73.14% examples, 43630 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:22:53,257 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:22:53,258 : INFO : EPOCH - 21 : training on 159102 raw words (61625 effective words) took 1.4s, 43830 effective words/s\n",
      "2019-12-06 13:22:54,314 : INFO : EPOCH 22 - PROGRESS: at 73.14% examples, 43664 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:22:54,667 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:22:54,667 : INFO : EPOCH - 22 : training on 159102 raw words (61792 effective words) took 1.4s, 43864 effective words/s\n",
      "2019-12-06 13:22:55,732 : INFO : EPOCH 23 - PROGRESS: at 73.14% examples, 43456 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:22:56,080 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:22:56,080 : INFO : EPOCH - 23 : training on 159102 raw words (61742 effective words) took 1.4s, 43754 effective words/s\n",
      "2019-12-06 13:22:57,147 : INFO : EPOCH 24 - PROGRESS: at 73.14% examples, 43360 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:22:57,499 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:22:57,499 : INFO : EPOCH - 24 : training on 159102 raw words (61840 effective words) took 1.4s, 43600 effective words/s\n",
      "2019-12-06 13:22:58,570 : INFO : EPOCH 25 - PROGRESS: at 73.14% examples, 43225 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:22:58,917 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:22:58,918 : INFO : EPOCH - 25 : training on 159102 raw words (61817 effective words) took 1.4s, 43588 effective words/s\n",
      "2019-12-06 13:22:59,973 : INFO : EPOCH 26 - PROGRESS: at 73.14% examples, 43759 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:00,326 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:00,327 : INFO : EPOCH - 26 : training on 159102 raw words (61747 effective words) took 1.4s, 43871 effective words/s\n",
      "2019-12-06 13:23:01,387 : INFO : EPOCH 27 - PROGRESS: at 73.14% examples, 43558 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:01,737 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:01,738 : INFO : EPOCH - 27 : training on 159102 raw words (61745 effective words) took 1.4s, 43786 effective words/s\n",
      "2019-12-06 13:23:02,793 : INFO : EPOCH 28 - PROGRESS: at 73.14% examples, 43721 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:03,153 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:03,154 : INFO : EPOCH - 28 : training on 159102 raw words (61759 effective words) took 1.4s, 43666 effective words/s\n",
      "2019-12-06 13:23:04,210 : INFO : EPOCH 29 - PROGRESS: at 73.14% examples, 43850 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:04,559 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:04,560 : INFO : EPOCH - 29 : training on 159102 raw words (61788 effective words) took 1.4s, 43964 effective words/s\n",
      "2019-12-06 13:23:05,645 : INFO : EPOCH 30 - PROGRESS: at 73.14% examples, 42575 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:06,024 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:06,025 : INFO : EPOCH - 30 : training on 159102 raw words (61787 effective words) took 1.5s, 42184 effective words/s\n",
      "2019-12-06 13:23:07,105 : INFO : EPOCH 31 - PROGRESS: at 73.14% examples, 42687 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:07,472 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:07,472 : INFO : EPOCH - 31 : training on 159102 raw words (61693 effective words) took 1.4s, 42684 effective words/s\n",
      "2019-12-06 13:23:08,538 : INFO : EPOCH 32 - PROGRESS: at 73.14% examples, 43364 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:08,890 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:08,891 : INFO : EPOCH - 32 : training on 159102 raw words (61773 effective words) took 1.4s, 43570 effective words/s\n",
      "2019-12-06 13:23:09,950 : INFO : EPOCH 33 - PROGRESS: at 73.14% examples, 43671 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:10,301 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:10,302 : INFO : EPOCH - 33 : training on 159102 raw words (61847 effective words) took 1.4s, 43859 effective words/s\n",
      "2019-12-06 13:23:11,353 : INFO : EPOCH 34 - PROGRESS: at 73.14% examples, 43872 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-06 13:23:11,707 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:11,708 : INFO : EPOCH - 34 : training on 159102 raw words (61774 effective words) took 1.4s, 43988 effective words/s\n",
      "2019-12-06 13:23:12,765 : INFO : EPOCH 35 - PROGRESS: at 73.14% examples, 43715 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:13,116 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:13,117 : INFO : EPOCH - 35 : training on 159102 raw words (61704 effective words) took 1.4s, 43803 effective words/s\n",
      "2019-12-06 13:23:14,211 : INFO : EPOCH 36 - PROGRESS: at 73.14% examples, 42272 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:14,562 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:14,563 : INFO : EPOCH - 36 : training on 159102 raw words (61836 effective words) took 1.4s, 42788 effective words/s\n",
      "2019-12-06 13:23:15,621 : INFO : EPOCH 37 - PROGRESS: at 73.14% examples, 43640 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-06 13:23:15,970 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:15,970 : INFO : EPOCH - 37 : training on 159102 raw words (61736 effective words) took 1.4s, 43909 effective words/s\n",
      "2019-12-06 13:23:17,042 : INFO : EPOCH 38 - PROGRESS: at 73.14% examples, 43081 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:17,390 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:17,391 : INFO : EPOCH - 38 : training on 159102 raw words (61709 effective words) took 1.4s, 43478 effective words/s\n",
      "2019-12-06 13:23:18,438 : INFO : EPOCH 39 - PROGRESS: at 73.14% examples, 44088 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-06 13:23:18,791 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:18,791 : INFO : EPOCH - 39 : training on 159102 raw words (61826 effective words) took 1.4s, 44157 effective words/s\n",
      "2019-12-06 13:23:19,853 : INFO : EPOCH 40 - PROGRESS: at 73.14% examples, 43535 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-06 13:23:20,203 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:20,203 : INFO : EPOCH - 40 : training on 159102 raw words (61785 effective words) took 1.4s, 43811 effective words/s\n",
      "2019-12-06 13:23:21,257 : INFO : EPOCH 41 - PROGRESS: at 66.38% examples, 40044 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:21,689 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:21,690 : INFO : EPOCH - 41 : training on 159102 raw words (61637 effective words) took 1.5s, 41474 effective words/s\n",
      "2019-12-06 13:23:22,732 : INFO : EPOCH 42 - PROGRESS: at 73.14% examples, 44222 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:23,080 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:23,081 : INFO : EPOCH - 42 : training on 159102 raw words (61642 effective words) took 1.4s, 44349 effective words/s\n",
      "2019-12-06 13:23:24,124 : INFO : EPOCH 43 - PROGRESS: at 73.14% examples, 44223 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:24,477 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:24,478 : INFO : EPOCH - 43 : training on 159102 raw words (61668 effective words) took 1.4s, 44193 effective words/s\n",
      "2019-12-06 13:23:25,535 : INFO : EPOCH 44 - PROGRESS: at 73.14% examples, 43682 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:25,892 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:25,893 : INFO : EPOCH - 44 : training on 159102 raw words (61715 effective words) took 1.4s, 43645 effective words/s\n",
      "2019-12-06 13:23:26,945 : INFO : EPOCH 45 - PROGRESS: at 73.14% examples, 43945 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:27,290 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:27,290 : INFO : EPOCH - 45 : training on 159102 raw words (61739 effective words) took 1.4s, 44197 effective words/s\n",
      "2019-12-06 13:23:28,335 : INFO : EPOCH 46 - PROGRESS: at 73.14% examples, 44133 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:28,682 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:28,683 : INFO : EPOCH - 46 : training on 159102 raw words (61687 effective words) took 1.4s, 44326 effective words/s\n",
      "2019-12-06 13:23:29,724 : INFO : EPOCH 47 - PROGRESS: at 73.14% examples, 44350 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:30,072 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:30,072 : INFO : EPOCH - 47 : training on 159102 raw words (61632 effective words) took 1.4s, 44418 effective words/s\n",
      "2019-12-06 13:23:31,114 : INFO : EPOCH 48 - PROGRESS: at 73.14% examples, 44397 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:31,457 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:31,458 : INFO : EPOCH - 48 : training on 159102 raw words (61829 effective words) took 1.4s, 44637 effective words/s\n",
      "2019-12-06 13:23:32,506 : INFO : EPOCH 49 - PROGRESS: at 73.14% examples, 44034 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-06 13:23:32,853 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:32,854 : INFO : EPOCH - 49 : training on 159102 raw words (61761 effective words) took 1.4s, 44269 effective words/s\n",
      "2019-12-06 13:23:33,889 : INFO : EPOCH 50 - PROGRESS: at 73.14% examples, 44645 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:34,235 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:34,236 : INFO : EPOCH - 50 : training on 159102 raw words (61750 effective words) took 1.4s, 44755 effective words/s\n",
      "2019-12-06 13:23:35,277 : INFO : EPOCH 51 - PROGRESS: at 73.14% examples, 44315 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:35,640 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:35,641 : INFO : EPOCH - 51 : training on 159102 raw words (61691 effective words) took 1.4s, 43919 effective words/s\n",
      "2019-12-06 13:23:36,691 : INFO : EPOCH 52 - PROGRESS: at 73.14% examples, 43903 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:37,045 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:37,046 : INFO : EPOCH - 52 : training on 159102 raw words (61681 effective words) took 1.4s, 43930 effective words/s\n",
      "2019-12-06 13:23:38,125 : INFO : EPOCH 53 - PROGRESS: at 73.14% examples, 42965 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:38,485 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:38,486 : INFO : EPOCH - 53 : training on 159102 raw words (61814 effective words) took 1.4s, 42961 effective words/s\n",
      "2019-12-06 13:23:39,541 : INFO : EPOCH 54 - PROGRESS: at 73.14% examples, 43726 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:39,888 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:39,889 : INFO : EPOCH - 54 : training on 159102 raw words (61739 effective words) took 1.4s, 44032 effective words/s\n",
      "2019-12-06 13:23:40,924 : INFO : EPOCH 55 - PROGRESS: at 73.14% examples, 44638 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:41,265 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:41,266 : INFO : EPOCH - 55 : training on 159102 raw words (61665 effective words) took 1.4s, 44812 effective words/s\n",
      "2019-12-06 13:23:42,355 : INFO : EPOCH 56 - PROGRESS: at 73.14% examples, 42367 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:42,712 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:42,713 : INFO : EPOCH - 56 : training on 159102 raw words (61695 effective words) took 1.4s, 42683 effective words/s\n",
      "2019-12-06 13:23:43,720 : INFO : EPOCH 57 - PROGRESS: at 66.38% examples, 41948 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:44,167 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:44,167 : INFO : EPOCH - 57 : training on 159102 raw words (61780 effective words) took 1.5s, 42509 effective words/s\n",
      "2019-12-06 13:23:45,239 : INFO : EPOCH 58 - PROGRESS: at 73.14% examples, 43010 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:45,587 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:45,588 : INFO : EPOCH - 58 : training on 159102 raw words (61619 effective words) took 1.4s, 43399 effective words/s\n",
      "2019-12-06 13:23:46,659 : INFO : EPOCH 59 - PROGRESS: at 73.14% examples, 43267 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:47,020 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:47,021 : INFO : EPOCH - 59 : training on 159102 raw words (61866 effective words) took 1.4s, 43233 effective words/s\n",
      "2019-12-06 13:23:48,068 : INFO : EPOCH 60 - PROGRESS: at 73.14% examples, 44196 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:48,417 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:48,418 : INFO : EPOCH - 60 : training on 159102 raw words (61817 effective words) took 1.4s, 44270 effective words/s\n",
      "2019-12-06 13:23:49,454 : INFO : EPOCH 61 - PROGRESS: at 73.14% examples, 44644 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:49,795 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:49,796 : INFO : EPOCH - 61 : training on 159102 raw words (61860 effective words) took 1.4s, 44931 effective words/s\n",
      "2019-12-06 13:23:50,839 : INFO : EPOCH 62 - PROGRESS: at 73.14% examples, 44294 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:51,189 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:51,190 : INFO : EPOCH - 62 : training on 159102 raw words (61812 effective words) took 1.4s, 44379 effective words/s\n",
      "2019-12-06 13:23:52,209 : INFO : EPOCH 63 - PROGRESS: at 73.14% examples, 45277 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:52,559 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:52,560 : INFO : EPOCH - 63 : training on 159102 raw words (61752 effective words) took 1.4s, 45114 effective words/s\n",
      "2019-12-06 13:23:53,590 : INFO : EPOCH 64 - PROGRESS: at 73.14% examples, 45036 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:53,939 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:53,939 : INFO : EPOCH - 64 : training on 159102 raw words (61990 effective words) took 1.4s, 44961 effective words/s\n",
      "2019-12-06 13:23:55,012 : INFO : EPOCH 65 - PROGRESS: at 73.14% examples, 43152 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:55,356 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:55,357 : INFO : EPOCH - 65 : training on 159102 raw words (61888 effective words) took 1.4s, 43687 effective words/s\n",
      "2019-12-06 13:23:56,390 : INFO : EPOCH 66 - PROGRESS: at 73.14% examples, 44732 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:56,730 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:56,731 : INFO : EPOCH - 66 : training on 159102 raw words (61787 effective words) took 1.4s, 45030 effective words/s\n",
      "2019-12-06 13:23:57,763 : INFO : EPOCH 67 - PROGRESS: at 73.14% examples, 44842 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:58,107 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:58,108 : INFO : EPOCH - 67 : training on 159102 raw words (61856 effective words) took 1.4s, 44950 effective words/s\n",
      "2019-12-06 13:23:59,145 : INFO : EPOCH 68 - PROGRESS: at 66.38% examples, 40754 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:23:59,577 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:23:59,577 : INFO : EPOCH - 68 : training on 159102 raw words (61950 effective words) took 1.5s, 42170 effective words/s\n",
      "2019-12-06 13:24:00,603 : INFO : EPOCH 69 - PROGRESS: at 73.14% examples, 45018 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:00,944 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:00,944 : INFO : EPOCH - 69 : training on 159102 raw words (61791 effective words) took 1.4s, 45261 effective words/s\n",
      "2019-12-06 13:24:01,964 : INFO : EPOCH 70 - PROGRESS: at 73.14% examples, 45269 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:02,303 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:02,304 : INFO : EPOCH - 70 : training on 159102 raw words (61673 effective words) took 1.4s, 45375 effective words/s\n",
      "2019-12-06 13:24:03,366 : INFO : EPOCH 71 - PROGRESS: at 73.14% examples, 43448 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:03,736 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:03,737 : INFO : EPOCH - 71 : training on 159102 raw words (61698 effective words) took 1.4s, 43073 effective words/s\n",
      "2019-12-06 13:24:04,788 : INFO : EPOCH 72 - PROGRESS: at 73.14% examples, 44076 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:05,137 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:05,138 : INFO : EPOCH - 72 : training on 159102 raw words (61845 effective words) took 1.4s, 44215 effective words/s\n",
      "2019-12-06 13:24:06,209 : INFO : EPOCH 73 - PROGRESS: at 73.14% examples, 43183 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:06,553 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:06,554 : INFO : EPOCH - 73 : training on 159102 raw words (61773 effective words) took 1.4s, 43652 effective words/s\n",
      "2019-12-06 13:24:07,602 : INFO : EPOCH 74 - PROGRESS: at 73.14% examples, 43986 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:07,949 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:07,949 : INFO : EPOCH - 74 : training on 159102 raw words (61702 effective words) took 1.4s, 44232 effective words/s\n",
      "2019-12-06 13:24:08,970 : INFO : EPOCH 75 - PROGRESS: at 73.14% examples, 45305 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:09,308 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:09,309 : INFO : EPOCH - 75 : training on 159102 raw words (61758 effective words) took 1.4s, 45472 effective words/s\n",
      "2019-12-06 13:24:10,336 : INFO : EPOCH 76 - PROGRESS: at 73.14% examples, 45025 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:10,674 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:10,675 : INFO : EPOCH - 76 : training on 159102 raw words (61860 effective words) took 1.4s, 45321 effective words/s\n",
      "2019-12-06 13:24:11,724 : INFO : EPOCH 77 - PROGRESS: at 73.14% examples, 43963 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:12,070 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:12,071 : INFO : EPOCH - 77 : training on 159102 raw words (61640 effective words) took 1.4s, 44172 effective words/s\n",
      "2019-12-06 13:24:13,085 : INFO : EPOCH 78 - PROGRESS: at 73.14% examples, 45512 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:13,425 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:13,425 : INFO : EPOCH - 78 : training on 159102 raw words (61729 effective words) took 1.4s, 45605 effective words/s\n",
      "2019-12-06 13:24:14,448 : INFO : EPOCH 79 - PROGRESS: at 73.14% examples, 45073 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:14,800 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:14,801 : INFO : EPOCH - 79 : training on 159102 raw words (61639 effective words) took 1.4s, 44878 effective words/s\n",
      "2019-12-06 13:24:15,856 : INFO : EPOCH 80 - PROGRESS: at 73.14% examples, 43811 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:16,207 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:16,208 : INFO : EPOCH - 80 : training on 159102 raw words (61836 effective words) took 1.4s, 43954 effective words/s\n",
      "2019-12-06 13:24:17,274 : INFO : EPOCH 81 - PROGRESS: at 73.14% examples, 43434 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:17,620 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:17,621 : INFO : EPOCH - 81 : training on 159102 raw words (61949 effective words) took 1.4s, 43884 effective words/s\n",
      "2019-12-06 13:24:18,656 : INFO : EPOCH 82 - PROGRESS: at 73.14% examples, 44462 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:19,001 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:19,001 : INFO : EPOCH - 82 : training on 159102 raw words (61541 effective words) took 1.4s, 44595 effective words/s\n",
      "2019-12-06 13:24:20,024 : INFO : EPOCH 83 - PROGRESS: at 73.14% examples, 45209 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:20,365 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:20,366 : INFO : EPOCH - 83 : training on 159102 raw words (61688 effective words) took 1.4s, 45264 effective words/s\n",
      "2019-12-06 13:24:21,396 : INFO : EPOCH 84 - PROGRESS: at 73.14% examples, 44772 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:21,762 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:21,763 : INFO : EPOCH - 84 : training on 159102 raw words (61714 effective words) took 1.4s, 44191 effective words/s\n",
      "2019-12-06 13:24:22,817 : INFO : EPOCH 85 - PROGRESS: at 73.14% examples, 43703 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:23,173 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:23,173 : INFO : EPOCH - 85 : training on 159102 raw words (61637 effective words) took 1.4s, 43733 effective words/s\n",
      "2019-12-06 13:24:24,209 : INFO : EPOCH 86 - PROGRESS: at 73.14% examples, 44662 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:24,557 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:24,558 : INFO : EPOCH - 86 : training on 159102 raw words (61866 effective words) took 1.4s, 44736 effective words/s\n",
      "2019-12-06 13:24:25,599 : INFO : EPOCH 87 - PROGRESS: at 73.14% examples, 44471 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:25,951 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:25,951 : INFO : EPOCH - 87 : training on 159102 raw words (61838 effective words) took 1.4s, 44404 effective words/s\n",
      "2019-12-06 13:24:27,000 : INFO : EPOCH 88 - PROGRESS: at 73.14% examples, 44182 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:27,355 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:27,356 : INFO : EPOCH - 88 : training on 159102 raw words (62008 effective words) took 1.4s, 44178 effective words/s\n",
      "2019-12-06 13:24:28,386 : INFO : EPOCH 89 - PROGRESS: at 73.14% examples, 44977 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:28,721 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:28,722 : INFO : EPOCH - 89 : training on 159102 raw words (61818 effective words) took 1.4s, 45313 effective words/s\n",
      "2019-12-06 13:24:29,736 : INFO : EPOCH 90 - PROGRESS: at 73.14% examples, 45569 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:30,076 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:30,077 : INFO : EPOCH - 90 : training on 159102 raw words (61732 effective words) took 1.4s, 45579 effective words/s\n",
      "2019-12-06 13:24:31,081 : INFO : EPOCH 91 - PROGRESS: at 73.14% examples, 45939 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:31,413 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:31,414 : INFO : EPOCH - 91 : training on 159102 raw words (61668 effective words) took 1.3s, 46137 effective words/s\n",
      "2019-12-06 13:24:32,431 : INFO : EPOCH 92 - PROGRESS: at 73.14% examples, 45526 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:32,885 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:32,886 : INFO : EPOCH - 92 : training on 159102 raw words (61828 effective words) took 1.5s, 42066 effective words/s\n",
      "2019-12-06 13:24:33,903 : INFO : EPOCH 93 - PROGRESS: at 73.14% examples, 45507 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:34,240 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:34,241 : INFO : EPOCH - 93 : training on 159102 raw words (61805 effective words) took 1.4s, 45628 effective words/s\n",
      "2019-12-06 13:24:35,269 : INFO : EPOCH 94 - PROGRESS: at 73.14% examples, 44870 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:35,599 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:35,599 : INFO : EPOCH - 94 : training on 159102 raw words (61557 effective words) took 1.4s, 45355 effective words/s\n",
      "2019-12-06 13:24:36,621 : INFO : EPOCH 95 - PROGRESS: at 73.14% examples, 45271 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:36,957 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:36,957 : INFO : EPOCH - 95 : training on 159102 raw words (61766 effective words) took 1.4s, 45542 effective words/s\n",
      "2019-12-06 13:24:37,983 : INFO : EPOCH 96 - PROGRESS: at 73.14% examples, 45013 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:38,322 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:38,323 : INFO : EPOCH - 96 : training on 159102 raw words (61779 effective words) took 1.4s, 45274 effective words/s\n",
      "2019-12-06 13:24:39,359 : INFO : EPOCH 97 - PROGRESS: at 73.14% examples, 44498 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:39,700 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:39,701 : INFO : EPOCH - 97 : training on 159102 raw words (61604 effective words) took 1.4s, 44722 effective words/s\n",
      "2019-12-06 13:24:40,717 : INFO : EPOCH 98 - PROGRESS: at 73.14% examples, 45528 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:41,069 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:41,070 : INFO : EPOCH - 98 : training on 159102 raw words (61814 effective words) took 1.4s, 45165 effective words/s\n",
      "2019-12-06 13:24:42,088 : INFO : EPOCH 99 - PROGRESS: at 73.14% examples, 45368 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:42,423 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:42,423 : INFO : EPOCH - 99 : training on 159102 raw words (61668 effective words) took 1.4s, 45625 effective words/s\n",
      "2019-12-06 13:24:43,450 : INFO : EPOCH 100 - PROGRESS: at 73.14% examples, 45118 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-06 13:24:43,785 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:24:43,786 : INFO : EPOCH - 100 : training on 159102 raw words (61942 effective words) took 1.4s, 45480 effective words/s\n",
      "2019-12-06 13:24:43,786 : INFO : training on a 15910200 raw words (6176599 effective words) took 140.3s, 44022 effective words/s\n",
      "2019-12-06 13:24:43,797 : INFO : collecting all words and their counts\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-f63200fbe40f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0msaved_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"model.bin\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'models'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[1;34m(self, documents, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[0;32m   1182\u001b[0m         total_words, corpus_count = self.vocabulary.scan_vocab(\n\u001b[0;32m   1183\u001b[0m             \u001b[0mdocuments\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocvecs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m             \u001b[0mprogress_per\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprogress_per\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m         )\n\u001b[0;32m   1186\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[1;34m(self, documents, corpus_file, docvecs, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             \u001b[0mdocuments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTaggedLineDocument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1381\u001b[1;33m         \u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_scan_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocvecs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m         logger.info(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36m_scan_vocab\u001b[1;34m(self, documents, docvecs, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdocument_no\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchecked_string_types\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1310\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1311\u001b[0m                     logger.warning(\n\u001b[0;32m   1312\u001b[0m                         \u001b[1;34m\"Each 'words' should be a list of words (usually unicode strings). \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'words'"
     ]
    }
   ],
   "source": [
    "import gensim.models as g\n",
    "\n",
    "#vector_size = 300\n",
    "#window_size = 15\n",
    "#min_count = 1\n",
    "#sampling_threshold = 1e-5\n",
    "#negative_size = 5\n",
    "#train_epoch = 100\n",
    "#dm = 0 #0 = dbow; 1 = dmpv\n",
    "#worker_count = 1 #number of parallel processes\n",
    "\n",
    "train_corpus = \"data.txt\"\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "#docs = g.doc2vec.TaggedLineDocument(train_corpus)\n",
    "#model = g.Doc2Vec(docs, size=vector_size, window=window_size, min_count=min_count, sample=sampling_threshold, workers=worker_count, hs=0, dm=dm, negative=negative_size, dbow_words=1, dm_concat=1, iter=train_epoch)\n",
    "#saved_path = \"model.bin\"\n",
    "\n",
    "model.build_vocab(train_corpus)\n",
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=20)\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "model.save('models/doc2vec.model')\n",
    "model.save_word2vec_format('models/trained.word2vec')\n",
    "print('Done!')\n",
    "model.save(saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-06 13:34:19,369 : INFO : collecting all words and their counts\n",
      "2019-12-06 13:34:19,376 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2019-12-06 13:34:19,398 : INFO : collected 11097 word types and 1000 unique tags from a corpus of 1000 examples and 84408 words\n",
      "2019-12-06 13:34:19,399 : INFO : Loading a fresh vocabulary\n",
      "2019-12-06 13:34:19,414 : INFO : effective_min_count=1 retains 11097 unique words (100% of original 11097, drops 0)\n",
      "2019-12-06 13:34:19,415 : INFO : effective_min_count=1 leaves 84408 word corpus (100% of original 84408, drops 0)\n",
      "2019-12-06 13:34:19,454 : INFO : deleting the raw counts dictionary of 11097 items\n",
      "2019-12-06 13:34:19,454 : INFO : sample=1e-05 downsamples 3599 most-common words\n",
      "2019-12-06 13:34:19,454 : INFO : downsampling leaves estimated 22704 word corpus (26.9% of prior 84408)\n",
      "2019-12-06 13:34:19,475 : INFO : estimated required memory for 11097 words and 300 dimensions: 33381300 bytes\n",
      "2019-12-06 13:34:19,476 : INFO : resetting layer weights\n",
      "2019-12-06 13:34:21,841 : INFO : training model with 1 workers on 11098 vocabulary and 300 features, using sg=1 hs=0 sample=1e-05 negative=5 window=15\n",
      "2019-12-06 13:34:22,424 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:34:22,425 : INFO : EPOCH - 1 : training on 84408 raw words (23740 effective words) took 0.6s, 40772 effective words/s\n",
      "2019-12-06 13:34:22,981 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:34:22,982 : INFO : EPOCH - 2 : training on 84408 raw words (23650 effective words) took 0.6s, 42551 effective words/s\n",
      "2019-12-06 13:34:23,535 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:34:23,536 : INFO : EPOCH - 3 : training on 84408 raw words (23619 effective words) took 0.6s, 42689 effective words/s\n",
      "2019-12-06 13:34:24,096 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:34:24,096 : INFO : EPOCH - 4 : training on 84408 raw words (23652 effective words) took 0.6s, 42286 effective words/s\n",
      "2019-12-06 13:34:24,669 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:34:24,670 : INFO : EPOCH - 5 : training on 84408 raw words (23580 effective words) took 0.6s, 41202 effective words/s\n",
      "2019-12-06 13:34:25,242 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:34:25,243 : INFO : EPOCH - 6 : training on 84408 raw words (23768 effective words) took 0.6s, 41577 effective words/s\n",
      "2019-12-06 13:34:25,809 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:34:25,810 : INFO : EPOCH - 7 : training on 84408 raw words (23697 effective words) took 0.6s, 41911 effective words/s\n",
      "2019-12-06 13:34:26,377 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:34:26,378 : INFO : EPOCH - 8 : training on 84408 raw words (23643 effective words) took 0.6s, 41644 effective words/s\n",
      "2019-12-06 13:34:26,938 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:34:26,939 : INFO : EPOCH - 9 : training on 84408 raw words (23579 effective words) took 0.6s, 42086 effective words/s\n",
      "2019-12-06 13:34:27,518 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:34:27,519 : INFO : EPOCH - 10 : training on 84408 raw words (23674 effective words) took 0.6s, 40893 effective words/s\n",
      "2019-12-06 13:34:28,100 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:34:28,101 : INFO : EPOCH - 11 : training on 84408 raw words (23720 effective words) took 0.6s, 40865 effective words/s\n",
      "2019-12-06 13:34:28,673 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:34:28,674 : INFO : EPOCH - 12 : training on 84408 raw words (23711 effective words) took 0.6s, 41452 effective words/s\n",
      "2019-12-06 13:34:29,235 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:34:29,235 : INFO : EPOCH - 13 : training on 84408 raw words (23598 effective words) took 0.6s, 42080 effective words/s\n",
      "2019-12-06 13:34:29,786 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:34:29,786 : INFO : EPOCH - 14 : training on 84408 raw words (23691 effective words) took 0.5s, 43176 effective words/s\n",
      "2019-12-06 13:34:30,341 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:34:30,342 : INFO : EPOCH - 15 : training on 84408 raw words (23755 effective words) took 0.6s, 42851 effective words/s\n",
      "2019-12-06 13:34:30,892 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:34:30,893 : INFO : EPOCH - 16 : training on 84408 raw words (23605 effective words) took 0.6s, 42876 effective words/s\n",
      "2019-12-06 13:34:31,463 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:34:31,463 : INFO : EPOCH - 17 : training on 84408 raw words (23645 effective words) took 0.6s, 41519 effective words/s\n",
      "2019-12-06 13:34:32,026 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-06 13:34:32,027 : INFO : EPOCH - 18 : training on 84408 raw words (23656 effective words) took 0.6s, 42044 effective words/s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-4cf811e77bf2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m#train doc2vec model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTaggedLineDocument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msampling_threshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworker_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnegative_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbow_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdm_concat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m#save model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, documents, corpus_file, dm_mean, dm, dbow_words, dm_concat, dm_tag_count, docvecs, docvecs_mapfile, comment, trim_rule, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    617\u001b[0m                 \u001b[0mdocuments\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 619\u001b[1;33m                 end_alpha=self.min_alpha, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    620\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, documents, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[0;32m    811\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 813\u001b[1;33m             queue_factor=queue_factor, report_delay=report_delay, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1081\u001b[1;33m             **kwargs)\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[0;32m    552\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[0;32m    554\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[0;32m    487\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[0;32m    488\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m             \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# blocks if workers too slow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# a thread reporting that it finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\queue.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gensim.models as g\n",
    "import logging\n",
    "\n",
    "vector_size = 300\n",
    "window_size = 15\n",
    "min_count = 1\n",
    "sampling_threshold = 1e-5\n",
    "negative_size = 5\n",
    "train_epoch = 100\n",
    "dm = 0 \n",
    "worker_count = 1 \n",
    "\n",
    "\n",
    "train_corpus = \"train_docs.txt\"\n",
    "saved_path = \"model.bin\"\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#entrainement du doc2vec\n",
    "docs = g.doc2vec.TaggedLineDocument(train_corpus)\n",
    "model = g.Doc2Vec(docs, size=vector_size, window=window_size, min_count=min_count, sample=sampling_threshold, workers=worker_count, hs=0, dm=dm, negative=negative_size, dbow_words=1, dm_concat=1, iter=train_epoch)\n",
    "\n",
    "model.save(saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-06 10:21:38,273 : INFO : loading Doc2Vec object from model.bin\n",
      "2019-12-06 10:21:38,458 : INFO : loading vocabulary recursively from model.bin.vocabulary.* with mmap=None\n",
      "2019-12-06 10:21:38,459 : INFO : loading trainables recursively from model.bin.trainables.* with mmap=None\n",
      "2019-12-06 10:21:38,460 : INFO : loading wv recursively from model.bin.wv.* with mmap=None\n",
      "2019-12-06 10:21:38,460 : INFO : loading docvecs recursively from model.bin.docvecs.* with mmap=None\n",
      "2019-12-06 10:21:38,461 : INFO : loaded model.bin\n"
     ]
    }
   ],
   "source": [
    "import gensim.models as g\n",
    "import codecs\n",
    "\n",
    "model=\"model.bin\"\n",
    "test_docs=\"test_docs.txt\"\n",
    "output_file=\"test_vectors.txt\"\n",
    "\n",
    "start_alpha=0.01\n",
    "infer_epoch=1000\n",
    "model.build_vocab(text_docs)\n",
    "m = g.Doc2Vec.load(model)\n",
    "test_docs = [ x.strip().split() for x in codecs.open(test_docs, \"r\", \"utf-8\").readlines() ]\n",
    "output = open(output_file, \"w\")\n",
    "for d in test_docs:\n",
    "    output.write( \" \".join([str(x) for x in m.infer_vector(d, alpha=start_alpha, steps=infer_epoch)]) + \"\\n\" )\n",
    "output.flush()\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-06 10:21:41,337 : INFO : loading Doc2Vec object from model.bin\n",
      "2019-12-06 10:21:41,527 : INFO : loading vocabulary recursively from model.bin.vocabulary.* with mmap=None\n",
      "2019-12-06 10:21:41,528 : INFO : loading trainables recursively from model.bin.trainables.* with mmap=None\n",
      "2019-12-06 10:21:41,528 : INFO : loading wv recursively from model.bin.wv.* with mmap=None\n",
      "2019-12-06 10:21:41,528 : INFO : loading docvecs recursively from model.bin.docvecs.* with mmap=None\n",
      "2019-12-06 10:21:41,529 : INFO : loaded model.bin\n"
     ]
    }
   ],
   "source": [
    "model=\"model.bin\"\n",
    "test_docs=\"train_docs.txt\"\n",
    "output_file=\"train_vectors.txt\"\n",
    "\n",
    "start_alpha=0.01\n",
    "infer_epoch=1000\n",
    "\n",
    "m = g.Doc2Vec.load(model)\n",
    "test_docs = [ x.strip().split() for x in codecs.open(test_docs, \"r\", \"utf-8\").readlines() ]\n",
    "\n",
    "output = open(output_file, \"w\")\n",
    "for d in test_docs:\n",
    "    output.write( \" \".join([str(x) for x in m.infer_vector(d, alpha=start_alpha, steps=infer_epoch)]) + \"\\n\" )\n",
    "output.flush()\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-06 10:22:56,603 : INFO : collecting all words and their counts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doin cool math stuff...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-824c299c29f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyTexts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mdoc2vec_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mdoc2vec_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mdoc2vec_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoc2vec_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'models'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[1;34m(self, documents, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[0;32m   1182\u001b[0m         total_words, corpus_count = self.vocabulary.scan_vocab(\n\u001b[0;32m   1183\u001b[0m             \u001b[0mdocuments\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocvecs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m             \u001b[0mprogress_per\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprogress_per\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m         )\n\u001b[0;32m   1186\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[1;34m(self, documents, corpus_file, docvecs, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             \u001b[0mdocuments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTaggedLineDocument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1381\u001b[1;33m         \u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_scan_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocvecs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m         logger.info(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36m_scan_vocab\u001b[1;34m(self, documents, docvecs, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m   1306\u001b[0m         \u001b[0mchecked_string_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1308\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mdocument_no\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1309\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchecked_string_types\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1310\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-824c299c29f4>\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[0mTaggedDocument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msimple_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "import multiprocessing\n",
    "import os\n",
    "class MyTexts(object):\n",
    "    def __iter__(self):\n",
    "        for i in range(len(train_corpus)):\n",
    "            yield TaggedDocument(words=simple_preprocess(train_corpus['text'][i]), tags=[i])\n",
    "\n",
    "\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1\n",
    "print('Doin cool math stuff...')\n",
    "cores = multiprocessing.cpu_count()\n",
    "texts = MyTexts()\n",
    "doc2vec_model = Doc2Vec(size=200, workers=cores)\n",
    "doc2vec_model.build_vocab(texts)\n",
    "doc2vec_model.train(texts, total_examples=doc2vec_model.corpus_count, epochs=15)\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "    doc2vec_model.save('models/doc2vec.model')\n",
    "else:\n",
    "    doc2vec_model.save('models/doc2vec.model')\n",
    "print('And we did it!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot():\n",
    "    quit=False\n",
    "    while quit == False:\n",
    "        text = str(input('Me: '))\n",
    "        ##an optional quit command\n",
    "        if text == 'quit()':\n",
    "            quit=True\n",
    "        else:\n",
    "            tokens = text.split()\n",
    "            new_vector = model.infer_vector(tokens)\n",
    "            index = model.docvecs.most_similar([new_vector], topn = 10)\n",
    "            print('Chatbot: ' + train_data.iloc[index[0][0],1])\n",
    "            print('\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'most_similar'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-8f53cd2fe62f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moutput_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'need'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mchatbot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'most_similar'"
     ]
    }
   ],
   "source": [
    "output_file.most_similar(['need'])[0:2]\n",
    "chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "import multiprocessing\n",
    "import os\n",
    "class MyTexts(object):\n",
    "    def __iter__(self):\n",
    "        for i in range(len(train_data)):\n",
    "            yield TaggedDocument(words=simple_preprocess(train_data['text'][i]), tags=[train_data['id'][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-06 10:39:27,817 : INFO : collecting all words and their counts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[1;34m(self, documents, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[0;32m   1182\u001b[0m         total_words, corpus_count = self.vocabulary.scan_vocab(\n\u001b[0;32m   1183\u001b[0m             \u001b[0mdocuments\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocvecs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m             \u001b[0mprogress_per\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprogress_per\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m         )\n\u001b[0;32m   1186\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[1;34m(self, documents, corpus_file, docvecs, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             \u001b[0mdocuments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTaggedLineDocument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1381\u001b[1;33m         \u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_scan_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocvecs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m         logger.info(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36m_scan_vocab\u001b[1;34m(self, documents, docvecs, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m   1306\u001b[0m         \u001b[0mchecked_string_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1308\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mdocument_no\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1309\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchecked_string_types\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1310\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-46-2ea85393f04a>\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mMyTexts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mTaggedDocument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msimple_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1\n",
    "print('Training the model...')\n",
    "cores = multiprocessing.cpu_count()\n",
    "texts = MyTexts()\n",
    "doc2vec_model = Doc2Vec(vector_size=300, workers=cores, min_count=1, window=3, negative=5)\n",
    "doc2vec_model.build_vocab(texts)\n",
    "doc2vec_model.train(texts, total_examples=doc2vec_model.corpus_count, epochs=20)\n",
    "\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "doc2vec_model.save('models/doc2vec.model')\n",
    "doc2vec_model.save_word2vec_format('models/trained.word2vec')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Fore\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import display\n",
    "from ipywidgets import Output\n",
    "\n",
    "def chatbot():\n",
    "    quit=False\n",
    "    responses = []\n",
    "    \n",
    "    while quit == False:\n",
    "        text = str(input('Message: '))\n",
    "        if text == 'quit()':\n",
    "            quit=True\n",
    "        else:\n",
    "            tokens = text.split()\n",
    "            new_vector = output_file.infer_vector(tokens)\n",
    "            index = model.docvecs.most_similar([new_vector], topn = 10)\n",
    "            response = Fore.RED + 'Chatbot: ' + data.iloc[int(index[0][0])].response\n",
    "            responses.append(response)\n",
    "            out = Output()\n",
    "            display(out)\n",
    "            with out:\n",
    "                clear_output()\n",
    "                print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: i need help\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'infer_vector'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-aac704dbd9c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mchatbot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-66-8e76ffac6dcb>\u001b[0m in \u001b[0;36mchatbot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;31m##infer vector for text the model may not have seen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mnew_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[1;31m##find the most similar [i] tags\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_vector\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'infer_vector'"
     ]
    }
   ],
   "source": [
    "chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(model):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'wv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-d0f4ea6902bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtsne_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-52-6af7c76e2364>\u001b[0m in \u001b[0;36mtsne_plot\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mtokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'wv'"
     ]
    }
   ],
   "source": [
    "tsne_plot(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
